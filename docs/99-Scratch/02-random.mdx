import { BlueHighlight, GreenHighlight } from '@site/src/components/Highlight';

# Random Facts

I put random facts I find interesting in here, or something I want to remember.

## Latency and A/B Testing

A famous Amazon A/B test intentionally delayed page latency by **100ms** and found even small delays (not noticeable by blink of an eye) can have substantial impact on user engagement and revenue outcomes. This is also an example of high <BlueHighlight>elasticity</BlueHighlight>, where demand is very sensitive to small factors. If a site slows just slightly, there's a big shift in the user response and behavior. Thus, <GreenHighlight>there is a high elasticity of demand with respect to latency</GreenHighlight>.

### Elasticity
Elasticity can be thought of as synonymous with sensitivity.
Other economic examples of elasticity include **price elasticity** where raising the price of a product like coffee by 10 cents can have dramatic reduction in the number of sales.

The mathematical formulation of elasticity is:

$$
\text{Price elasticity} = \frac{\%\Delta \text{Quantity Demanded}}{\%\Delta \text{Price}}
$$

For elasticity $>1$ it is elastic (sensitive), elasticity $<1$ it is inelastic (insensitive) and $=1$ it is unit elastic (proportional).

## Power Law

The <BlueHighlight>power law</BlueHighlight> comes up quite a bit when it comes to discussing the scalability of AI performance (as well as its meaning in data vis). That's why it's important to understand the mathematics behind it, how to spot a power law, and what it really implies.

A power law defines the relationship between two quantities where one is proportional to a power of another, $Y = kX^{\alpha}$. A small change in $X$ can lead to a proportionally large change in $Y$. You can have positive alpha for explosive growth or negative alpha for inverse shrinking.

One attribute of power law is <BlueHighlight>scale invariance</BlueHighlight> where $f(\lambda x) = \lambda^{\Delta} f(x)$. Scaling $x$ by $c$ results in $c^{\alpha}$ in the $Y$. Consequently, the distribution or relationship looks the same regardless of the scale you use.

### Power Law Distribution

A power law distribution is one where the probability of an event is proportional to the power of the magnitude of the event, so $p(\text{event}) = k \cdot \text{magnitude}^{-\alpha}$ where $\alpha > 0$. Note that the convention is for a negative alpha because the probability decays as a power of $x$. Compared to a normal distribution, a power law distribution has a **fat tail** so the probability of extreme events is higher than a normal distribution. An example is in the startup world where the unicorns dominate wealth creation and return on investment. The average case does not matter, because all the value comes from the tail.

![powerlaw](/img/powerlawdist.png)

### AI Power Law

In the context of AI you have the power law relationship between loss and compute such that $L(C) \approx kC^{-\alpha}$. On a log to log plot, it will be a straight line or linear relationship. But in actually loss quantities, the reduction is sublinear, isn't that bad? No - the inverse power law here means that you can always keep reducing the loss although the gains get less with the flattening of the curve. With more compute you will always get predictable linear gain in the log to log scale!
