import { BlueHighlight, GreenHighlight } from '@site/src/components/Highlight';

# Debugging

A collection of issues I've consistently encountered over time, and the fixes for them.

## Exit code `137`

Usually indicates some sort of Out-Of-Memory (OOM) error. Was the case on my HPC job when my `np.load` was allocating close to 100GB in memory and getting the process killed. The tricky thing here is that due to being killed there is very little in terms of execution trace for debugging.

On the Swing cluster, while total node memory might be alrge ~1TB, job memory is allocated based on fraction of cores. So requesting `ncpus=32` only gets 120GB. You can check with:

```bash
qstat -f $PBS_JOBID | grep mem
```

## `channel 3: open failed`

While working with Jupyter Notebooks on HPC clusters, I've gotten this confusing message many times after a kernel dies. I start seeing annoying lines consistently printed to my ssh terminal reading:

```channel 3: open failed: connect failed: Connection refused```

Without any other information, it can be hard to know how to get this error to stop printing, which can clutter the terminal.

Recently I learned what is happening is that Jupyter notebooks in VSCode in order to work must connect to **Jupyer servers**,aka a Python process running `jupyter notebook` or `jupyter lab`. If you run locally, VSCode starts a local server in the background. In HPC environments, you do this by pasting in the remote Jupyter server URL you get from running `jupyter lab --no-browser --port=8866` to VSCode e.g. `http://localhost:8866/?token=abcd`. VScode will save the URL to a **remote server list** so it can quickly reconnect. However, if the URL becomes stale, VSCode does not know that and will continue attempting to connect, resulting in the error.

The easy fix is to open up the command palette `Ctrl + Shift + P` and click the option `Jupyter: Clear Remote Server List`.

## `$PATH` Altered

When certain commands like `qsub` or `qstat` suddenly stop working because they cannot be found, the likely culprit is an altered `$PATH` environment variable. The `$PATH` is a list of directories where the system looks for executable programs when you type a command. For example, if you type `ls` the system look at `$PATH` and find it inside of `/usr/bin`.

Check by running `echo $PATH`. Specifically for pbsnodes commands, it probably came from `/opt/pbs/bin` being removed.

In a clean shell on LCRC, my path list is:

```txt
PATH=/home/dma/miniconda3/bin:/home/dma/miniconda3/condabin:/usr/lpp/mmfs/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/pbs/bin:/home/dma/.local/bin:/home/dma/bin
```

## PyTorch CPU Contention

**Torch by default in any process sets the number of threads to the number of system cores.** This can unknowingly lead to massive CPU contention if you have multiple torch processes running.

I found out about this bug while trying to parallelize `torch` inference with multiprocessing. Turns out that `torch` operations in worker processes by default sets `num_threads` to the total number of cores on the system. Consequently, the time for model inference actually increases across all processes as they end up fighting over the cpu resources.

Specifically what happened was my job was allocated 128 ncpus. Then with each process that I created, it set the `torch` number of threads by default to 128. So each of say four processes attempted to perform multithreaded `torch` operations using 128 threads at the same time. As a result, there was a massive slow down that was near ~40x.

The fix was simply at worker process initialization to run:

```python
torch.set_num_threads(8) # a small number
```

## Slow HPC Writes

For the preprocessing script for the SAR floodmapping dataset with 8000 tiles, I needed to make it robust and efficient for hundreds of gigabytes of patches. A strategy I devised is to have parallel workers load and patch the tiles in chunks, and save each chunk to file. The chunks would then be streamed to a final memory mapped array. This strategy allowed for producing 300GB+ `.npy` files with limited RAM.

While the parallel patching and chunking was fast, I encountered a bottleneck with subsequently writing the chunks to a large memmapped array. The writes would happen sequentially. The problematic code would take close to two hours to finish for two chunk files with a total combined size of 30GB:

```python
for tmp_file in chunk_files:
    with np.load(tmp_file) as arr:
        n_patches = arr.shape[0]

        # this part is incredibly slow for large 10GB+ chunk files!!!!
        final_arr[patch_offset:patch_offset + n_patches, ...] = arr
        patch_offset += n_patches
```

One possibility for this bottleneck is the networking bandwidth of 

At this point I realize I need to learn about the HPC system. More specifically the scratch space, GPFS filesystems.
