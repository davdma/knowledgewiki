import { BlueHighlight, GreenHighlight } from '@site/src/components/Highlight';

# Debugging

A collection of issues I've consistently encountered over time, and the fixes for them.

## Exit code `137`

Usually indicates some sort of Out-Of-Memory (OOM) error. Was the case on my HPC job when my `np.load` was allocating close to 100GB in memory and getting the process killed. The tricky thing here is that due to being killed there is very little in terms of execution trace for debugging.

On the Swing cluster, while total node memory might be alrge ~1TB, job memory is allocated based on fraction of cores. So requesting `ncpus=32` only gets 120GB. You can check with:

```bash
qstat -f $PBS_JOBID | grep mem
```

## `channel 3: open failed`

While working with Jupyter Notebooks on HPC clusters, I've gotten this confusing message many times after a kernel dies. I start seeing annoying lines consistently printed to my ssh terminal reading:

```channel 3: open failed: connect failed: Connection refused```

Without any other information, it can be hard to know how to get this error to stop printing, which can clutter the terminal.

Recently I learned what is happening is that Jupyter notebooks in VSCode in order to work must connect to **Jupyer servers**,aka a Python process running `jupyter notebook` or `jupyter lab`. If you run locally, VSCode starts a local server in the background. In HPC environments, you do this by pasting in the remote Jupyter server URL you get from running `jupyter lab --no-browser --port=8866` to VSCode e.g. `http://localhost:8866/?token=abcd`. VScode will save the URL to a **remote server list** so it can quickly reconnect. However, if the URL becomes stale, VSCode does not know that and will continue attempting to connect, resulting in the error.

The easy fix is to open up the command palette `Ctrl + Shift + P` and click the option `Jupyter: Clear Remote Server List`.

## `$PATH` Altered

When certain commands like `qsub` or `qstat` suddenly stop working because they cannot be found, the likely culprit is an altered `$PATH` environment variable. The `$PATH` is a list of directories where the system looks for executable programs when you type a command. For example, if you type `ls` the system look at `$PATH` and find it inside of `/usr/bin`.

Check by running `echo $PATH`. Specifically for pbsnodes commands, it probably came from `/opt/pbs/bin` being removed.

In a clean shell on LCRC, my path list is:

```txt
PATH=/home/dma/miniconda3/bin:/home/dma/miniconda3/condabin:/usr/lpp/mmfs/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/pbs/bin:/home/dma/.local/bin:/home/dma/bin
```

## PyTorch CPU Contention

**Torch by default in any process sets the number of threads to the number of system cores.** This can unknowingly lead to massive CPU contention if you have multiple torch processes running.

I found out about this bug while trying to parallelize `torch` inference with multiprocessing. Turns out that `torch` operations in worker processes by default sets `num_threads` to the total number of cores on the system. Consequently, the time for model inference actually increases across all processes as they end up fighting over the cpu resources.

Specifically what happened was my job was allocated 128 ncpus. Then with each process that I created, it set the `torch` number of threads by default to 128. So each of say four processes attempted to perform multithreaded `torch` operations using 128 threads at the same time. As a result, there was a massive slow down that was near ~40x.

The fix was simply at worker process initialization to run:

```python
torch.set_num_threads(8) # a small number
```

## Slow HPC GPFS Writes

For the preprocessing script for the SAR floodmapping dataset with 8000 tiles, I needed to make it robust and efficient for hundreds of gigabytes of patches. A strategy I devised is to have parallel workers load and patch the tiles in chunks, and save each chunk to file. The chunks would then be streamed to a final memory mapped array. This strategy allowed for producing 300GB+ `.npy` files with limited RAM.

While the parallel patching and chunking was fast, I encountered a bottleneck with subsequently writing the chunks to a large memmapped array. The writes would happen sequentially. The problematic code would take close to two hours to finish for two chunk files with a total combined size of 30GB:

```python
final_arr = np.lib.format.open_memmap(output_file, mode="w+", dtype=np.float32, shape=(total_patches, S2_DATASET_CHANNELS, size, size))

for tmp_file in chunk_files:
    with np.load(tmp_file) as arr:
        n_patches = arr.shape[0]

        # this part is incredibly slow for large 10GB+ chunk files!!!!
        final_arr[patch_offset:patch_offset + n_patches, ...] = arr
        patch_offset += n_patches
```

One possibility for this bottleneck is the networking bandwidth of GPFS on the Argonne clusters. I was making page by page writes to the filesystem by using my project directory `/lcrc/project/hydrosm/dma/data/preprocess`. Instead, I realized that I can greatly speedup the latency by using the local scratch space (~1TB in size) which is attached to the compute node. Since the harddrive is directly attached to the compute node and does not use GPFS, it is much much faster. With `findmnt` I learned that `/scratch` is mounted as a high performance XFS filesystem.

The difference was staggering. While the original implementation for around 30GB of data without using the scratch disk took roughly 1200s (20 minutes) of runtime, the new implementation using the scratch disk took roughly 20s of runtime. **This was a speedup of 60x!**

:::info[Key Idea]

The point of the **scratch directory** is to have a locally mounted harddrive on the node for fast job related data storage. Since it is physically mounted, it has much better performance than using the GPFS storage, which is separate from the node and lives on a dedicated storage server cluster like `/lcrc`. <GreenHighlight>The network latency of communicating between servers adds a lot of overhead compared to simply storing files temporarily on scratch disk.</GreenHighlight>

Here is a good [summary](https://ucsf-ti.github.io/tipcc-web/good-practices/using-local-scratch.html):

> All nodes have their own local storage mounted as `/scratch`. The `/scratch` storage is fast - faster than system-wide storage such as home folders and lab storage - which make it ideal for holding intermediate data files. This will also lower the load on the system-wide storage and the local network. Using local `/scratch` is a win-win for everyone.

With scratch directories, you do not need to worry about deleting files and cleaning up during the job, as it is automatically cleared once the job ends.

I asked Jeremy whether there is a specific HPC doc that would teach you these things, but he said that he learned this all through trial and error. The docs might only allude to these things, so it would talk about the scratch directory, but only by looking more into why it is there, do you learn about these problems.

:::

At this point I realize I need to learn about the HPC system. More specifically the scratch space, GPFS filesystems.
