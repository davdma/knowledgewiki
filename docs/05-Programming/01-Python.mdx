import { BlueHighlight, GreenHighlight } from '@site/src/components/Highlight';

# Python

## The Interpreter

The Python bytecode interpreter e.g. what is installed on `/usr/local/bin/Python3.13` can read and execute commands interactively or read and execute Python scripts.

**How does Python code run?**

First the python code is compiled into bytecode and stored as `.pyc` files in `__pycache__`. An example might be:

```python
print("Hello")
```

Compiles to

```txt
LOAD_NAME     0 (print)
LOAD_CONST    0 ('hello')
CALL_FUNCTION 1
POP_TOP
LOAD_CONST    1 (None)
RETURN_VALUE
```

The Python interpreter program consists of a loop (bytecode evaluation loop) that iterates over bytecode instructions, fetching one bytecode instruction at a time. It decodes the bytecode instruction, and executes the corresponding C function. This is also referred to as the <BlueHighlight>CEval loop</BlueHighlight>.

:::info[Key Idea]

The flow of execution for CPython is:

source code -> bytecode -> interpreter loop execution

**Each bytecode instruction maps to a function in `ceval.c`.**

:::

A very useful source from Python docs: [the bytecode interpreter](https://github.com/python/cpython/blob/main/InternalDocs/interpreter.md).

## Iterables, Iterators, Generators

An <BlueHighlight>iterable</BlueHighlight> is any object that can return an iterator.

An <BlueHighlight>iterator</BlueHighlight> is a helper object that produces the next value given an internal state. An iterator can be thought of as a lazy **value factory** that provides a value only when requested. It is possible for iterators to produce infinite sequences and never terminate.

:::info[Key Idea]

An **iterable** implements `__iter__()` which returns an iterator object, and an **iterator** implements `__next__()` and returns the next value, raising `StopIteration` exception when no more elements are available.

You can manually get the iterator from an iterable using `iter()` and the next value from an iterator using `next()`.

:::

For instance, an iterator might be implemented using two distinct patterns. With pattern A, you have both the iterable and the iterator as one class - **the iterable is its own iterator**. In this case it can only be consumed once since the iterable maintains iterator state itself. In pattern B, you have a separate iterable and iterator, with `__iter__` returning a fresh iterator each time (this one is more Pythonic and canonical).

```python
# Pattern A: Iterable is the iterator
class ChunkedRange:
    def __init__(self, rangeStart, rangeStop, chunkSize):
        self.rangeStart = rangeStart
        self.rangeStop = rangeStop
        self.chunkSize = chunkSize
        self.cur_val = rangeStart

    def __iter__(self):
        return self

    def __next__(self):
        if self.cur_val >= self.rangeStop:
            raise StopIteration

        lst = []
        stop_val = self.cur_val + self.chunkSize
        stop_val = min(stop_val, self.rangeStop)
        while self.cur_val < stop_val:
            lst.append(self.cur_val)
            self.cur_val += 1

        return lst


# Pattern B: Iterable and iterator are separate
class ChunkedRangeIterable:
    def __init__(self, rangeStart, rangeStop, chunkSize):
        self.rangeStart = rangeStart
        self.rangeStop = rangeStop
        self.chunkSize = chunkSize

    def __iter__(self):
        return ChunkedRangeIterator(self.rangeStart, self.rangeStop, self.chunkSize)


class ChunkedRangeIterator:
    def __init__(self, rangeStart, rangeStop, chunkSize):
        self.rangeStart = rangeStart
        self.rangeStop = rangeStop
        self.chunkSize = chunkSize
        self.cur_val = rangeStart

    # An iterator is also an iterable!
    def __iter__(self):
        return self

    def __next__(self):
        if self.cur_val >= self.rangeStop:
            raise StopIteration

        lst = []
        stop_val = min(self.cur_val + self.chunkSize, self.rangeStop)
        while self.cur_val < stop_val:
            lst.append(self.cur_val)
            self.cur_val += 1
        return lst
```

Canonically, iterator classes even if it is defined separately from the iterable should still implement `__iter__()` and return `self`. This is because in Python <GreenHighlight>an iterator is an iterable over itself</GreenHighlight> and calling `iter()` on an iterator should returns itself (even if it is partially consumed).

:::note[Detail]

Iterables can return themselves as the iterator, by implementing both the `__iter__()` and the `__next__()` method, so `__iter__()` returns `self`.

Calling `iter()` on an iterator should also return itself.

:::

So what happens during a **for loop**? The for loop calls `iter()` on the target object to get its iterator and then exhausts it. The syntax `for x in iterable` calls `iter(iterable)` to get an iterator, and then exhausts the iterator until `StopIteration` is thrown. Take the code `for x in range(0, 10)`. Here, the object `range(0, 10)` is an iterable which can provide an iterator when `iter(range(0, 10))` is called.

A <BlueHighlight>generator</BlueHighlight> is a **subset of iterators**. They exist to provide a shortcut to building iterators, by allowing you to write an iterator without needing an explicit `__iter__()` and `__next__()` methods. It can take the form of a **generator function** (with `yield`) or a **generator expression**.

For instance, you can write a generator function like so:

```python
def firstn(n):
    num = 0
    while num < n:
        yield num
        num += 1

sum_of_first_n = sum(firstn(100000))
```

<GreenHighlight>When a generator function is called, it returns an iterator known as a generator</GreenHighlight>.

A yield expression in any function body turns it into a generator function. Think of the `yield` as a `return` that does not end the function, it simply suspends the state. The next time the generator function executes, it proceeds from where it was last paused. When the generator reaches the end of the function or a return statement, it raises `StopIteration`.

Hence, this will return the sequence `1, 2, 3`:

```python
def foo():
    yield 1
    yield 2
    yield 3
```

A cool quirk of generator functions is that they are not just for one way communication from the generator to the caller with `next()`. They also enable the caller to communicate with the generator to modify its execution via a `send()`. A `yield` expression actually will take on the sent value after resuming. Thus generators are **coroutines**.

<details>
<summary>What is a coroutine?</summary>

A coroutine is a program that allows execution to be paused and then resumed later.

Python has bidirectional coroutines. They allow bidirectional communication at **suspension points** so that it is possible to resume execution with new information.
</details>

```python
def echo(value=None):
    while True:
        try:
            # here the expression (yield value) can change based on send
            value = (yield value)
        except Exception as e:
            value = e
```

You can also create generator expressions which are additional shortcuts to build generators out of expressions similar to list comprehensions:

```python
# list comprehension
doubles = [2 * n for n in range(1000)]

# list from generator comprehension
doubles = list(2 * n for n in range(1000))
```

:::info[Key Idea]

**A generator is a special type of iterator**, but iterators are not necessarily generators. Generators provide a quick and easy way to create iterators and build lazy generation of values with the main advantage being low memory usage.

:::

:::tip[Trick]

Generators are powerful tools to make your code more memory and CPU efficient. Whenever you see:

```python
def func():
    result = []
    for x in ...:
        result.append(x)
    return result
```

Try to replace it with:

```python
def iterator_func():
    for x in ...:
        yield x

# when you need the full list just do
result = list(iterator_func())
```

:::

For more info, consult this helpful resource with diagrams [here](https://nvie.com/posts/iterators-vs-generators/).

## `zip()`

A very useful utility function in Python is `zip()` which takes any number of iterables, and <GreenHighlight>combines the iterables into one **iterator** of tuples</GreenHighlight>. Once the shortest iterable is exhausted the iterator terminates (hence the iterables do not have to be the same length everywhere).

```python
# Example from my flood mapping project
channels = [True, False, True, True, True]
channel_names = ["red", "green", "blue", "nir", "swir"]

for item in zip(channels, channel_names):
    print(item)

# (True, "red")
# (False, "green")
# (True, "blue")
# ...
```

To unzip a list of tuples, the inverse of the `zip()` operation is `zip(*iterable)`, which returns the original groups as tuples:

```python
zipped_list = [(1, "one"), (2, "two"), (3, "three")]

nums, words = zip(*zipped_list)
print(nums)
# (1, 2, 3)
print(words)
# ("one", "two", "three")
```

A more pythonic and less error free (since `zip(*iterable)` cannot handle an empty list) just use list comprehension:

```python
lst1 = [x[0] for x in zipped_list]
lst2 = [x[1] for x in zipped_list]
```

## `map()`

The function takes the form `map(function, iterable, strict=True)` and returns **an iterator that applies the function** to every element of the input iterable.

Multiple iterables can also be provided, in which case the function must take as many arguments and applies to the iterables in parallel. If `strict=True` then if one of the multiple iterables gets exhausted before others, a `ValueError` gets raised.

## Class Attributes

As opposed to instance attributes, class attributes are shared across all instances of a class:

```python
class Dog:
    species = "Canis familiaris" # class attribute

    def __init__(self, name):
        self.name = name # instance attribute
```

Class attributes are also mutable, but this is not safe. By convention, class attributes are used for immutable things, while instance attributes serve to store mutable things.

## Decorators

What are python decorators for?

The motivation of decorators in Python is <GreenHighlight>to apply some transformation to a function or method</GreenHighlight>. This allows for simple modification or extension of the function behavior (think of it as taking a function, wrapping it in additional functionality, and replacing the original function). For instance, the snippet:

```python
@dec1
@dec2
def func(arg1, arg2, ...):
    pass
```

is no different in its intent than the snippet:
```python
def func(arg1, arg2, ...):
    pass
func = dec2(dec1(func))
```

**Decorators can be stacked, which can be thought of as composing them together!** For instance,

```python
@decomaker(argA, argB, ...)
def func(arg1, arg2, ...):
    pass
```

is equivalent to:

```python
func = decomaker(argA, argB, ...)(func)
```

Here are the most common decorators you should know:
* `@property` - turns function into property or attribute style getter e.g. `Circle.area` to get the value
* `@classmethod` - use `cls` instead of `self` to call function, so receives the class itself as the first argument. Usually this is used for alternative constructor functions that provide other ways to instantiate an object - you pass in `cls` then it returns `cls(name, int(age))` for example.
* `@staticmethod` - defines method without `self` handle that lives in the class namespace so it doesn't depend on instance or class e.g. `MathUtils.add(3, 4)`
* `@abstractmethod` - makes a method that must be implemented

## Abstract Base Classes (ABC)

An Abstract Base Class has the property that it cannot be instantiated directly, only inherited like an interface. A class that inherits from an ABC must implement **all of its abstract methods**.

```python
from abc import ABC, abstractmethod

class Animal(ABC):
    # every subclass of Animal must implement speak()
    @abstractmethod
    def speak(self):
        pass
```

Abstract classes can still have regular class attributes and methods that get inherited by children:

```python
class Animal(ABC):
    species = "Unknown" # class attribute
    
    def __init__(self, name):
        self.name = name # instance attribute
```

## Data Classes

The python dataclasses decorator `dataclass` allows you to create structs (will automatically generate boilerplate `__init__` constructors). That way you save yourself having to create an `__init__` with all the struct arguments and then storing them in `self.arg = arg`.

The way it works is that the decorator will look for **fields** in the class which are **class variables with type annotations**, and generates the methods with the fields in the order they were defined in the class.

```python
from dataclasses import dataclass

@dataclass
class Box:
    name: str
    unit_price: float
    quantity: int

    def total_value(self) -> float:
        return self.unit_price * self.quantity

    # Automatically generated so we don't need this:
    # def __init__(self, name: str, unit_price: float, quantity: int):
        # self.name = name
        # self.unit_price = unit_price
        # self.quantity = quantity
```

### `field`

You will find it useful to also import `field` method from `dataclasses` module.

A `field` is by default implicitly run for each annotated class variable defined in the `dataclass`. A class variable `name: str` is implicitly treated as `name: str = field()` with no arguments, so it's default value is set to None. When you initialize a class variable `y: int = 5` in the class definition, the default is passed into the `field` method like `y: int = field(default=5)`.

A point of confusion to avoid is whether the varariable is shared among all instances or particular to an instance variable. See the following nuance:

```python
@dataclass
class Person:
    names: List[str] = []

p1 = Person()
p2 = Person()
p1.names.append("Dave")

print(p2.names) # ['Dave'] - it's shared!
```

But if we do this it is unique to each instance:

```python
@dataclass
class Person:
    names: List[str] = field(default_factory=list) # not [] as it expects a callable

p1 = Person()
p2 = Person()
p1.names.append("Dave")

print(p2.names) # [] Not shared!
```

What happens is that in the first example the list gets created at class definition time, resulting in each of the dataclass instances inheriting the reference to the same list object. In the second example the list gets created during instance construction, so the list for each instance is unique.

:::warning[Pitfall]

**Python class attributes:**

In learning about this, I think I had a misconception about regular Python class attributes, in that once they are initialized, they will always be shared by instances. If the instances tried to override them, the replaced value is still shared across variables.

The reality is that the initialized value at class definition time is shared among instances, but instances are free to override the attribute namespace and set a value for themselves (that does not get propagated to the other instances)!

At class definition time, any attributes assigned at the top without `__init__` become class attributes. When instances lookup attributes, they first check their own `__dict__` and if not found will default to the attribute until it gets shadowed.

Thus `@dataclass` does not break the rules of regular Python when it comes to the behavior of class variables and instance variables. It just generates boiler plate code `__init__`, `__repr__`, `__eq__` etc. so you do not have to.

:::

Now if you want to specify kwargs for the field beyond the default, you can pass that in explicity with options such as `default`, `default_factory`, `init`, `repr`:

```python
from dataclasses import dataclass, field

@dataclass
class Box:
    name: str
    unit_price: float
    quantity: int = field(repr=False) # field not included in string representation
    width: int = field(repr=False, default=5)
```

:::note[Detail]

The `dataclasses` `field` is not to be confused with the Pydantic `Field`. The concept is the same and Pydantic fields are also similarly defined by type annotations with customization with `Field`. The point of <BlueHighlight>field</BlueHighlight> across both cases is to represent a well designed class attribute for data storage and manipulation.

:::

## Python Packages

One thing I have learned when working with Python projects is that is important before starting to think about the audience and the environment you want to run the program in. If you want to deploy and share your code for other developers or users, having this figured out before hand saves a lot of headache later on. Otherwise you will find yourself in a mess trying to restructure your project into a proper package way down the line.

<BlueHighlight>Packages</BlueHighlight> helps consolidate a growing collection of modules or python scripts, so you have dotted module names, e.g. `A.B` for submodule `B` in package `A`. The `__init__.py` scripts make directories containing the file as packages.

### `__init__.py`

In the simplest form, `__init__.py` can be empty - **it marks directory it is in as a python package**. You can also add initialization code inside `__init__.py` to import certain functions/classes or run setup code when the package is imported. <GreenHighlight>A key functionality to allow you to setup a package's namespace upon import.</GreenHighlight>

The `__all__` inside a package's `__init__.py` is interpreted as a list of module names to be imported with the statement `from package import *`. The purpose is simply to allow the `import *` syntax, otherwise it is unnecessary.

```python

# from sound.effects import * -> imports three submodules
__all__ = ["echo", "surround", "reverse"]

```

What about the other stuff typically in a `__init__.py` file? When you add these statements into the `__init__.py`, the submodules are explicitly loaded when the package is imported:

```python

"""
AI Flood Detection Package

This package contains modules for flood detection using satellite imagery,
including data preprocessing, model architectures, training, and inference.
"""

__version__ = "1.0.0"
__author__ = "Hydrosm DMA Team"

# Make key components easily importable
from . import models
from . import utils
from . import training
from . import preprocess
from . import sampling
from . import inference
from . import benchmarking
from . import tuning

```

With empty `__init__.py`, python behavior is lazy and will only import the module from the package and run it when it is explicitly imported. If you add `from .training import train_s2`, then `train_s2` is a `floodmaps` attribute i.e. accessible with `floodmaps.train_s2`. Consequently it makes things easier to import from a top level like `from floodmaps import train_s2`. Similarly, if you do `from . import models` inside `__init__.py`, now it is loaded in under `floodmaps`. Instead of `import floodmaps.models` you can simply do `import floodmaps` and automatically have `floodmaps.models` bound in its namespace. You could also do `from floodmaps import models` - without it, this will throw an import error!

It can be confusing at first but it's all about bringing things into scope. A simple `import X` binds the module to the name `X`. A `from X import Y` looks inside the namespace of `X` and adds `Y` to the current scope. If you want something directly inside namespace without needing to do `floodmaps.training` you will have to explicitly request it. Hence the `__init__.py` autoimporting can be a useful feature.

If there is a submodule multiple directories deep, to make it more accessible at top level,you can add `from .models.training.utils.package import function` in `__init__.py`. Then do `from floodmaps import function` in any script you want. Reduces the problem of a long `from floodmaps.models.training.utils.package import function` import line.

```python

# Expose commonly used utilities
from .utils.config import Config
from .utils.utils import DATA_DIR, RESULTS_DIR, MODELS_DIR

```

Adding this allows you to do `from floodmaps import Config, DATA_DIR, RESULTS_DIR, MODELS_DIR` instead of `from floodmaps.utils.config import Config` etc.

:::warning[Pitfall]

In modern python with implicit namespace packages, `__init__.py` is often not needed as folders will automatically be treated as packages if you try to import from it. However, it's still better to have it in order to be explicit about your code as a package. You'll probably want to customize the symbols in the namespace using `__init__.py` anyways.

:::

One really useful thing for example when starting out is to pip install the repo as editable `pip install -e`. This allows you to sidestep annoying relative imports in a multiscript project.

### `PYTHONPATH`

The `PYTHONPATH` is an environment variable commonly set <GreenHighlight>to tell the python interpreter where to look for modules and packages before standard locations</GreenHighlight>, extending the import search path. From the python docs it "augments the default search path for module files". However, if you have your python project configured as a package, you can install it with `pip install -e .` which adds your package path so it can be found.

```bash

export PYTHONPATH=/home/user/myproject

```

## Closures

A <BlueHighlight>closure</BlueHighlight> happens with a function defined inside another function. The inner function grabs the outer variables defined in its enclosing lexical scope and remembers them even when the function is executed outside of that scope - the combination of a function and its enclosing scope is a closure. This is common in functional programming languages and is the case in Javascript as well.

For instance, if you returned this inner function, it still remembers the variable `name` in its surrounding scope.
```bash
def outer_func():
    name = "Pythonista"
    def inner_func():
        print(f"Hello {name}")

    return inner_func

greeter = outer_func()
greeter()
# Hello, Pythonista
```

## Parallelization

There are different main ways to parallelize code in Python: `multiprocessing` and `concurrent.futures` packages.

:::info[Key Ideas]

- `multiprocessing` is designed for parallelism for **processes** to bypass the GIL.
- `concurrent.futures` provides simpler API for both **thread** and **process** parallelsim.
The package `concurrent.futures` is designed to be a more consistent API around threading and processes. In most cases, `concurrent.futures` can be used instead of `multiprocessing`.

:::

Within `concurrent.futures` you will commonly use `ThreadPoolExecutor` and `ProcessPoolExecutor`.

An important distinction when designing parallelization in Python is thinking about whether the task is I/O bound or CPU bound. If it is I/O bound the recommendation is to use `ThreadPoolExecutor` and threads that share memory as you can bypass the GIL lock.

### Global Interpreter Lock (GIL)

The <BlueHighlight>Global Interpreter Lock</BlueHighlight> in Python is a mutex (lock) that allows only one thread to hold control of the Python interpreter and execute Python bytecode in a single process at one time. Hence even if your process spawns multiple threads, only one thread can execute at one time, which bottlenecks CPU bound multithreaded code.

**Why does this exist?**

The main reason for the GIL is to ensure memory safety. CPython uses <BlueHighlight>reference counting</BlueHighlight> to manage memory, where a reference count **variable** tracks the number of references to an object. When the count goes to zero, the memory is released.

Locks are needed to protect the reference count variable from race conditions which can leak memory or cause in-use memory to be released early. A solution is to have a lock for atomic updates to each reference counter, but adding a lock for each object can lead to too many acquisitions and releases of locks or deadlock situations. The solution was therefore a singular lock over the interpreter called the GIL.

:::info[Key Idea]

The Global Interpreter Lock is necessary because CPython memory management (with respect to reference count variables) is **not thread-safe**.

:::

Since we cannot truly have multithreading in Python, the common approach is to use **multiprocessing**, where you use multiple processes instead of threads. With multiprocessing, <GreenHighlight>each Python process gets its own Python interpreter, memory space, and GIL</GreenHighlight>. It enables parallelism while adding slightly more overhead than multithreaded approach.

:::info[Key Idea]

A notable exception is that **the GIL lock is released during I/O operations**, so I/O bound threads can benefit from multithreading in Python. Thus, multithreading is only bottlenecked by the GIL when the program is spending most of its time interpreting CPython bytecode.

It is also the case that **NumPy releases the GIL for low level operations**, hence a lot of numpy operations can run in parallel with multithreading.

:::

### Context and Start Methods

Important to both `multiprocessing` and `concurrent.futures` (which uses `multiprocessing` throughout) with respect to running multiple processes is the different process start methods. Understanding this will be useful to properly employing `ProcessPoolExecutor`. There are three ways:

1. `spawn` - The parent process starts a **fresh interpreter process** so the only resources inherited are what's needed for the process object's `run()` method.
2. `fork` - The parent process uses `os.fork()` to fork the Python interpreter. <GreenHighlight>This means that all memory from the parent process is inherited</GreenHighlight> including the Python interpreter, loaded modules, and objects (see below for details). For multiple processes, they can share the same pages in memory (with **Copy-On-Write**) as they are marked read-only. It is dangerous to fork for multithreaded processes.
3. `forkserver` - A hybrid between `fork` and `spawn` that reduces overhead of spawning workers. The concept is simple: have a separate Python server with simpler state `fork()` itself when a new process is needed. Each time you start a child, it asks the "fork server" process to fork a child and run its target callable.

![pythonmp](/img/multiprocessing.png)

<details>
<summary>What happens during a **fork**?</summary>

With an `os.fork()` call, you are calling the underlying C `fork` implementation [here](https://man7.org/linux/man-pages/man2/fork.2.html). `fork()` creates a new process by **duplicating the calling process**. The child process is the exact copy of the parent process except the child process has its unique PID. The child and parent processes have their own virtual memory address space. However, <GreenHighlight>at the time of call, both memory spaces map to the same physical content or pages.</GreenHighlight>

Things to remember:
- The entire virtual address space of the parent is **replicated** in the child via a copied page table.
- The child inherits a duplicate of the parent's file descriptor table.
- `fork()` is implemented with <BlueHighlight>copy-on-write</BlueHighlight> (COW) pages. This means only **the page table and PTEs of the parent is copied** and not the actual underlying memory pages. Furthermore, the **shared memory pages in both parent and child page tables are marked read-only**.
- `fork` is **problematic when called in a multithreaded process**, because the child process will only be a single thread, the thread that called `fork()`. But all of the memory including memory of the other threads will still get duplicated including the state of their stack, locks, data structures etc. Say you have Thread A and Thread B. Thread A calls `fork()` while Thread B is holding a mutex. Then after the fork the child process still has access to this mutex which got duplicated, but it appears held while the thread holding it has vanished. If the child process attempts to use the lock you get a **deadlock**.

**Copy-on-write:**
- Allows for efficient forking because copying the entire memory of parent process is too costly an operation.
- Why are pages marked read-only in **both parent and child**? Should it not just be the child? Since both parent and child share the mappings to the same physical page, you want to make sure that for isolation, neither of them must see the other's modifications. Hence, when the parent or the child modifies a page, a new page must be allocated.
</details>

<details>
<summary>What happens during **spawn**?</summary>

To create a fresh child process without the parent process' memory, spawn first **forks the current process, followed immediately by `exec`** to replace itself with a new Python instance.

`exec()` replaces the current <BlueHighlight>process image</BlueHighlight> (program code, data, heap, stack, PCB) with an entirely new process image, with the argument as the file to be executed.
</details>

### `multiprocessing.Queue`

An extremely important component of the `multiprocessing` Python package is the `Queue` class. The queue structure is the fundamental way to share data between processes. The queue allows for items to be added with `put()` and retrieved with `get()` with items retrieved in FIFO order.

**Everything that goes through the queue must be picklable.** As per the docs:

> Note that the restrictions on functions and arguments needing to picklable as per multiprocessing.Process apply when using submit() and map() on a ProcessPoolExecutor.

:::info[Key Idea]

Arguments passed into process workers while using `multiprocessing` or `concurrent.futures` are serialized and hence must be picklable.

:::

## `matplotlib.pyplot`

### Transformations

There are multiple coordinate systems in `matplotlib`:

- data coordinate system (`ax.transData`)
- axes coordinate system (`ax.transAxes`)
- subfigure coordinate system (`subfigure.transSubfigure`)
- figure coordinate system (`fig.transFigure`)
- display coordinate system (`None` or `IdentityTransform()`)

These `Transform objects` are naive to source and destination coordinate systems - <GreenHighlight>they take inputs in their coordinate system and transform the input to the **display** coordinate system</GreenHighlight>. Thus display coordinate system is `None` as the input is already in display coordinates.

Transformations can invert themselves with `Transform.inverted` in order to generate a transform from the output coord system (display) to the input coord system. For example, `ax.transData.inverted()` transforms display to data coordinates.

Transforms are important because they specify what coordinate system you are using, and map it to the display.
