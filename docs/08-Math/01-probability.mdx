import { BlueHighlight, GreenHighlight } from '@site/src/components/Highlight';

# Probability

## Probability Density Function

:::tip[Key Idea]

The CDF of a continuous random variable $X$ denoted $F_X(x)$ is the function:

$$
F_X(x) = P[X \leq x], x \in \mathbb{R}
$$

The PDF of $X$ denoted $f_X(x)$ is defined in relation to the CDF as:

$$
f_X(x) = \frac{d}{dx} F_X(x), x \in \mathbb{R}
$$

:::

While $f_X(x)$ by itself is not a probability (it can be greater than 1), it can have useful information about the probability. Take a look at the following equation:

$$
f_X(x) = \lim_{h \to 0} \frac{F_X(x + h) - F_X(x)}{h} = \lim_{h \to 0} \frac{P[x < X \leq x + h]}{h}
$$

The intuition for PDFs is that you can think of the PDF value at any point as <GreenHighlight>the probability that $X$ will be in a small neighborhood $(x, x+h]$ of the point $x$ relative to the size of that interval $h$.</GreenHighlight> Hence why it is also called the **density** of random variable, as the random variable is most likely to take on values where the probability is dense in that region.

## Power Law

:::warning[Pitfall]

It is easy to get confused by "power" and "exponential" relationships. A power relationship has the variable in the base $x^\alpha$ whereas an exponential relationship has the variable in the exponent $\alpha^x$.

:::

The <BlueHighlight>power law</BlueHighlight> comes up quite a bit when it comes to discussing the scalability of AI performance (as well as its meaning in data vis). That's why it's important to understand the mathematics behind it, how to spot a power law, and what it really implies.

A power law defines the relationship between two quantities where one is proportional to a power of another, $Y = kX^{\alpha}$. A small change in $X$ can lead to a proportionally large change in $Y$. You can have positive alpha for explosive growth or negative alpha for inverse shrinking.

One attribute of power law is <BlueHighlight>scale invariance</BlueHighlight> where $f(\lambda x) = \lambda^{\Delta} f(x)$. Scaling $x$ by $c$ results in $c^{\alpha}$ in the $Y$. Consequently, the distribution or relationship looks the same regardless of the scale you use.

### Power Law Distribution

A power law distribution is one where the probability of an event is proportional to the power of the magnitude of the event, so $p(\text{event}) = k \cdot \text{magnitude}^{-\alpha}$ where $\alpha > 0$. Note that the convention is for a negative alpha because the probability decays as a power of $x$. Compared to a normal distribution, a power law distribution has a **fat tail** so the probability of extreme events is higher than a normal distribution. An example is in the startup world where the unicorns dominate wealth creation and return on investment. The average case does not matter, because all the value comes from the tail.

![powerlaw](/img/powerlawdist.png)

### AI Power Law

In the context of AI you have the power law relationship between loss and compute such that $L(C) \approx kC^{-\alpha}$. On a log to log plot, it will be a straight line or linear relationship. But in actually loss quantities, the reduction is sublinear, isn't that bad? No - the inverse power law here means that you can always keep reducing the loss although the gains get less with the flattening of the curve. With more compute you will always get predictable linear gain in the log to log scale!

## Log-Uniform Distribution

![loguniform](/img/loguniform.png)

The reciprocal distribution also known as the log uniform distribution, is a continuous probability distribution, with the PDF defined as:

$$
f(x; a, b) = \frac{1}{x[\ln(a) - \ln(b)]}
$$

For $a \leq x \leq b$ and $a > 0$.

To best understand the log-uniform distribution, first start with the definition of the continuous uniform distribution. We say that $X$ is a uniform random variable on the interval $[a,b]$ denoted $X \sim U[a, b]$ if its cumulative distribution function (CDF) is equal to:

$$
F_X(x) =
\begin{cases}
1 &x > b \\
\frac{x-a}{b-a} &a \leq x \leq b \\
0 &x < a
\end{cases}
$$

The probability density function (PDF) for the uniform distribution $f_X(x)$ is just the derivative $f_X(x) = \frac{d}{dx}F_X(x)$:

$$
f_X(x) =
\begin{cases}
0 &x > b \\
\frac{d}{dx}\Big(\frac{x-a}{b-a}\Big) = \frac{1}{b-a} &a \leq x \leq b \\
0 &x < a
\end{cases}
$$

The reason why it is called log-**UNIFORM** is that a positive random variable $X$ is log-uniformly distributed if the logarithm of $X$ is uniformly distributed between $\ln(a)$ and $\ln(b)$:

$$
\ln(X) \sim U(\ln(a), \ln(b))
$$

To derive the formula for the log-uniform distribution, start with the CDF, which by definition should be uniform for the random variable $\ln(x)$ from $\ln(a)$ to $\ln(b)$:

$$
F_X(x) = \frac{\ln(x) - \ln(a)}{\ln(b) - \ln(a)}
$$

Now we take the derivative to get:

$$
f_X(x) = \frac{d}{dx}\Big(\frac{\ln(x)-\ln(a)}{\ln(b) - \ln(a)}\Big) = \frac{1}{x}\cdot\frac{1}{\ln(b) - \ln(a)} = \frac{1}{x[\ln(b) - \ln(a)]}
$$
